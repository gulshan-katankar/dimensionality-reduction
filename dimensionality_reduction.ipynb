{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201478c4",
   "metadata": {},
   "source": [
    "many machine learning models have thousands and even millions of features for each training instance some are of no use like this is called curse of dimensionality \n",
    "as we saw in the mnist dataset the white pixels arent much important (see fig 7-6) for the classification we can drop it from the training set and it wont be a problem, additionally the neighbouring features are highly correlated so we can merge it to a single pixel by taking the mean of the two pixel intensities, you will not lose much information \n",
    "\n",
    "reducing dimensionality does cause some loss in the training data even though it will speed up the training , it may make ur system performance slightly worse and your pipeline will be complex and thus difficult to maintain \n",
    "its always recommended to train your data with the original training set first before considering using dimensionality reduction\n",
    "in some cases it may filter out noise in the dataset and result in higher performance but in general it wont , it will just speed up the training \n",
    "\n",
    "dimensionality reduction is also useful when it comes to visualisations\n",
    "reducing the no of dimensions to 2 or 3 helps us to plot a graph for a high dimensional training set and gain some imp insights by visually detecting patterns such as clusters \n",
    "data visualisation also helps us in conveying our analysis and ideas to ppl who dont know data analysis \n",
    "\n",
    "first we will go through the curse of dimensionality reduction and understand what goes on in high dimensional space \n",
    "then we will discuss about two main approaches to dimensionality reduction projection and manifold learning and we will go through three most popular DR techniques PCA, random projection and locally linear embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aaeee9",
   "metadata": {},
   "source": [
    "THE CURSE OF DIMENSIONALITY :\n",
    "\n",
    "we are used to see the objects in 3d space so much that we are not able to visualize them in high dimensional space see fig 8-1\n",
    "\n",
    "if you pick a random point in a unit square we will have only a 0.4% chance of being located in 0.001 distance from the border \n",
    "but in a 10000 dimensional unit hypercube, the probab is greater than 99.99999% most points are close to the border in a high dimensional hypercube \n",
    "\n",
    "In low dimensions, borders are “thin,” so most points live comfortably in the middle.\n",
    "In high dimensions, borders make up almost the entire volume — the interior “shrinks.”\n",
    "This is counterintuitive because we expect random points to be “spread out,” but instead they concentrate near the boundaries.\n",
    "\n",
    "if we pick two points randomly in a unit square the distance between these points will be on average 0.52 \n",
    "if you pick these points in a 3d unit cube it wil be around 0.66 \n",
    "but if you pick it in a 1000000 dimensional unit hypercube it will be 408.25 roughly \n",
    "there are plenty of space in a high dimensional space as a result the high dimensional training set are at risk of being very sparse which means most of the training instances will be far away from each other, making predictions less reliable \n",
    "more dimensions the training set have more the risk of overfitting it \n",
    "\n",
    "in theory theres one solution for curse of dimensionality :\n",
    "increase the no of training set such that it reach the sufficient density of training instances \n",
    "but unfortunately the no of training instances required to reach a particular density grows exponentially with the no of dimensions, lets say for 100 features you will need training instances more than the atoms in observable universe(for mnist dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f66bf",
   "metadata": {},
   "source": [
    "MAIN APPROACHES FOR DIMENSIONALITY REDUCTION :\n",
    "\n",
    "Before we dive into specific dimensionality reduction algorithms, let’s take a look at the two main approaches to reducing dimensionality: projection and manifold learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f07dc",
   "metadata": {},
   "source": [
    "PROJECTION :\n",
    "\n",
    "in most of the real world problems training instances are not spread uniformly across all dimensions some features are constant and some are highly correlated as discussed in mnist \n",
    "as a result all the training instances lie within or close to a lower dimensional subspace see fig 8-2 :\n",
    "you can see 3d dataset represented by small spheres notice that all the training instances lie close to a plane this is a lower dimensional 2d subspace of higher dimensional 3d space \n",
    "if we project each datapoint perpendicullarly to this subspace we get a plane with all the points (see fig 8-3) and there u go we reduced the datasets dimension from 3d to 2d\n",
    "z1 and z2 are the coordinates of the projection on the plane "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20ae3d1",
   "metadata": {},
   "source": [
    "MANIFOLD LEARNING :\n",
    "\n",
    "projection is not always the best approach for DR in many cases the subspace may twist and turn for eg : see fig 8-4 :\n",
    "the swiss roll toy dataset \n",
    "simply projecting this into the plane will overlap the diff layers of the dataset together (see fig 8-5 left)\n",
    "what we can do is to unroll the swiss roll to obtain the 2d dataset on the right side of the figure 8-5\n",
    "\n",
    "the swiss roll is an example of a 2d manifold, in general a 2d manifold is a 2d shape which can be bent and rolled in a higher dimensional space \n",
    "more generally a d dimensional manifold is a part of n dimensional space where d < n, in case of the swiss roll d = 2 and n = 3\n",
    "\n",
    "many dr algorithms work by modelling the manifold on which the training data is placed, this is called manifold learning, it relies on the manifold assumption also called manifold hypothesis which says that most of the high dimensional real world dataset lies close to a much lower dimensional manifold, this observayion universally observed in real life \n",
    "\n",
    "lets see an example \n",
    "in mnist dataset each image is of 784 pixels that means it is in 784 dimensional space \n",
    "but in practice digits are mostly connected with strokes and \n",
    "borders are mostly white, \n",
    "digits are centered in the image \n",
    "shapes follow human handwriting patterns \n",
    "so only tiny fraction of 784 pixels or dimensions looks like digits \n",
    "so imagine if we generate random images OF 784 PIXELS almost none of them will look like a digit they will look like a noise \n",
    ", because the digits follow  a constraint strokes must be continuous borders mostly blank \n",
    "this means the space of valid digits is much smaller than the full image space \n",
    "in digit images all pixels are not allowed each pixel is correlated (if one pixels is black the neighbour pixel can also be black) they have less degree of freedom than random images \n",
    "these constraints squeeze the dataset into much lower dimensional manifold inside the higher dimensional space \n",
    "for eg the mnist dataset is of 784 d but in actual it is believed to be in 30 to 50 d \n",
    "\n",
    "the manifold assumption is often accompanied by another statement that says any task regression or classification becomes more simpler if expressed in lower dimensional space, see fig 8-6 :\n",
    "on the top you can see the the swiss roll is split into 2 classes, in 3d on left, the dataset decision bounday is more complex than the decision boundary in 2d space for the same dataset on the right which is just a straight line \n",
    "\n",
    "but this statement is not always true \n",
    "for eg : see fig 8-6 : bottom :\n",
    "the dataset in 3d space have a clear decision boundary at x=5 but in 2d space it gets divided into 4 diff parts more complex decision boundary \n",
    "\n",
    "in short reducing the dimensionality of your training set before training a model will usually speed up the training but it may not always lead to a better or simpler sol it all depends on the dataset \n",
    "\n",
    "now lets dive into some algorithms for dimensionality reduction "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
