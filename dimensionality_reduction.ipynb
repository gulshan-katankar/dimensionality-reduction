{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201478c4",
   "metadata": {},
   "source": [
    "many machine learning models have thousands and even millions of features for each training instance some are of no use like this is called curse of dimensionality \n",
    "as we saw in the mnist dataset the white pixels arent much important (see fig 7-6) for the classification we can drop it from the training set and it wont be a problem, additionally the neighbouring features are highly correlated so we can merge it to a single pixel by taking the mean of the two pixel intensities, you will not lose much information \n",
    "\n",
    "reducing dimensionality does cause some loss in the training data even though it will speed up the training , it may make ur system performance slightly worse and your pipeline will be complex and thus difficult to maintain \n",
    "its always recommended to train your data with the original training set first before considering using dimensionality reduction\n",
    "in some cases it may filter out noise in the dataset and result in higher performance but in general it wont , it will just speed up the training \n",
    "\n",
    "dimensionality reduction is also useful when it comes to visualisations\n",
    "reducing the no of dimensions to 2 or 3 helps us to plot a graph for a high dimensional training set and gain some imp insights by visually detecting patterns such as clusters \n",
    "data visualisation also helps us in conveying our analysis and ideas to ppl who dont know data analysis \n",
    "\n",
    "first we will go through the curse of dimensionality reduction and understand what goes on in high dimensional space \n",
    "then we will discuss about two main approaches to dimensionality reduction projection and manifold learning and we will go through three most popular DR techniques PCA, random projection and locally linear embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aaeee9",
   "metadata": {},
   "source": [
    "THE CURSE OF DIMENSIONALITY :\n",
    "\n",
    "we are used to see the objects in 3d space so much that we are not able to visualize them in high dimensional space see fig 8-1\n",
    "\n",
    "if you pick a random point in a unit square we will have only a 0.4% chance of being located in 0.001 distance from the border \n",
    "but in a 10000 dimensional unit hypercube, the probab is greater than 99.99999% most points are close to the border in a high dimensional hypercube \n",
    "\n",
    "In low dimensions, borders are ‚Äúthin,‚Äù so most points live comfortably in the middle.\n",
    "In high dimensions, borders make up almost the entire volume ‚Äî the interior ‚Äúshrinks.‚Äù\n",
    "This is counterintuitive because we expect random points to be ‚Äúspread out,‚Äù but instead they concentrate near the boundaries.\n",
    "\n",
    "if we pick two points randomly in a unit square the distance between these points will be on average 0.52 \n",
    "if you pick these points in a 3d unit cube it wil be around 0.66 \n",
    "but if you pick it in a 1000000 dimensional unit hypercube it will be 408.25 roughly \n",
    "there are plenty of space in a high dimensional space as a result the high dimensional training set are at risk of being very sparse which means most of the training instances will be far away from each other, making predictions less reliable \n",
    "more dimensions the training set have more the risk of overfitting it \n",
    "\n",
    "in theory theres one solution for curse of dimensionality :\n",
    "increase the no of training set such that it reach the sufficient density of training instances \n",
    "but unfortunately the no of training instances required to reach a particular density grows exponentially with the no of dimensions, lets say for 100 features you will need training instances more than the atoms in observable universe(for mnist dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f66bf",
   "metadata": {},
   "source": [
    "MAIN APPROACHES FOR DIMENSIONALITY REDUCTION :\n",
    "\n",
    "Before we dive into specific dimensionality reduction algorithms, let‚Äôs take a look at the two main approaches to reducing dimensionality: projection and manifold learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f07dc",
   "metadata": {},
   "source": [
    "PROJECTION :\n",
    "\n",
    "in most of the real world problems training instances are not spread uniformly across all dimensions some features are constant and some are highly correlated as discussed in mnist \n",
    "as a result all the training instances lie within or close to a lower dimensional subspace see fig 8-2 :\n",
    "you can see 3d dataset represented by small spheres notice that all the training instances lie close to a plane this is a lower dimensional 2d subspace of higher dimensional 3d space \n",
    "if we project each datapoint perpendicullarly to this subspace we get a plane with all the points (see fig 8-3) and there u go we reduced the datasets dimension from 3d to 2d\n",
    "z1 and z2 are the coordinates of the projection on the plane "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20ae3d1",
   "metadata": {},
   "source": [
    "MANIFOLD LEARNING :\n",
    "\n",
    "projection is not always the best approach for DR in many cases the subspace may twist and turn for eg : see fig 8-4 :\n",
    "the swiss roll toy dataset \n",
    "simply projecting this into the plane will overlap the diff layers of the dataset together (see fig 8-5 left)\n",
    "what we can do is to unroll the swiss roll to obtain the 2d dataset on the right side of the figure 8-5\n",
    "\n",
    "the swiss roll is an example of a 2d manifold, in general a 2d manifold is a 2d shape which can be bent and rolled in a higher dimensional space \n",
    "more generally a d dimensional manifold is a part of n dimensional space where d < n, in case of the swiss roll d = 2 and n = 3\n",
    "\n",
    "many dr algorithms work by modelling the manifold on which the training data is placed, this is called manifold learning, it relies on the manifold assumption also called manifold hypothesis which says that most of the high dimensional real world dataset lies close to a much lower dimensional manifold, this observayion universally observed in real life \n",
    "\n",
    "lets see an example \n",
    "in mnist dataset each image is of 784 pixels that means it is in 784 dimensional space \n",
    "but in practice digits are mostly connected with strokes and \n",
    "borders are mostly white, \n",
    "digits are centered in the image \n",
    "shapes follow human handwriting patterns \n",
    "so only tiny fraction of 784 pixels or dimensions looks like digits \n",
    "so imagine if we generate random images OF 784 PIXELS almost none of them will look like a digit they will look like a noise \n",
    ", because the digits follow  a constraint strokes must be continuous borders mostly blank \n",
    "this means the space of valid digits is much smaller than the full image space \n",
    "in digit images all pixels are not allowed each pixel is correlated (if one pixels is black the neighbour pixel can also be black) they have less degree of freedom than random images \n",
    "these constraints squeeze the dataset into much lower dimensional manifold inside the higher dimensional space \n",
    "for eg the mnist dataset is of 784 d but in actual it is believed to be in 30 to 50 d \n",
    "\n",
    "the manifold assumption is often accompanied by another statement that says any task regression or classification becomes more simpler if expressed in lower dimensional space, see fig 8-6 :\n",
    "on the top you can see the the swiss roll is split into 2 classes, in 3d on left, the dataset decision bounday is more complex than the decision boundary in 2d space for the same dataset on the right which is just a straight line \n",
    "\n",
    "but this statement is not always true \n",
    "for eg : see fig 8-6 : bottom :\n",
    "the dataset in 3d space have a clear decision boundary at x=5 but in 2d space it gets divided into 4 diff parts more complex decision boundary \n",
    "\n",
    "in short reducing the dimensionality of your training set before training a model will usually speed up the training but it may not always lead to a better or simpler sol it all depends on the dataset \n",
    "\n",
    "now lets dive into some algorithms for dimensionality reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a2b95",
   "metadata": {},
   "source": [
    "PCA \n",
    "\n",
    "principal component analyis, one of the most popular DR algo it finds the closest hyperplane to the dataset and then projects the datset on that hyperplane just like fog 8-2\n",
    "\n",
    "preserving the variance :\n",
    "\n",
    "before projecting the datset on a lower dimensional hyperplane u need to choose the right hyperplane for ex see fig 8-7 :\n",
    "there are 3 different axes the data is projected on each of those the top one have preserves most of the variance than the other two \n",
    "thats why its imp to select the right hyperplane it ensures we lose less information while projection \n",
    "its the axis that minimizes the mean squared distance between the dataset and the projection onto that axis \n",
    "\n",
    "\n",
    "principal components :\n",
    "\n",
    "see fig 8-7 \n",
    "in the left plot the PCA finds the principal components very clearly :\n",
    "\n",
    "first princpal component (c1) - the solid black line the direction with max variance in the data \n",
    "\n",
    "second principal component (c2) - the dotted line which is perpendicular to c1, capturing the largest possible left variance after accounting for c1 \n",
    "\n",
    "on the right you can see the how much of the data is projected on each of these \n",
    "\n",
    "if there was a high dimensional dataset pca would have find a third axis orthogonal to both c1 and c2 and then it may kept increasing depending on the dimensions in the dataset \n",
    "\n",
    "the i'th axis is called the i'th principal component(PC) in fig 8-7 the pc1 is the axis in which c1 lies and pc2 is the axis on which c2 lies  \n",
    "\n",
    "\n",
    "now for each pc pca computes a unit vector (length = 1), that is zero centered (goes throught the origin), points in the direction of maximum variance\n",
    "So, if your data is 3-dimensional, PCA will return 3 such orthogonal unit vectors ‚Äî one for each PC.\n",
    "any unit vector has two opposite directions eg  ùë£‚Éó and -ùë£‚Éó , lying on the same axis, both directions represent the same axis with maximum variance \n",
    "\n",
    "If two PCs capture almost the same amount of variance, their order is not stable:\n",
    "They might swap places (PC1 becomes PC2 and vice versa).\n",
    "They might even rotate together, producing two new orthogonal directions in the same plane.\n",
    "Even though the vectors change, the subspace (the plane or space they span) stays the same.\n",
    "\n",
    "we can find the PC of a training set using standard matrix factorisation technique called singular value decomposition (SVD) that can decompose the training matrix X into a matrix multiplication of matrices U Œ£ V^‚ä∫ where v contains the vectors that defined all the principal components see eq 8-1 copy 1\n",
    "\n",
    "the following python code uses numpy's svd function to obtain all the principal components of the 3d training set, then it extracts the two unit vectors that defines the first two pc's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eed08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Generate a 3D dataset lying close to a 2D plane\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 60 random 2D points\n",
    "n_samples = 60\n",
    "X_2D = np.random.randn(n_samples, 2)\n",
    "\n",
    "# Define a linear transformation to map them to 3D\n",
    "# This \"lifts\" the 2D data into 3D space\n",
    "W = np.array([[0.6, -0.4],\n",
    "              [0.3,  0.8],\n",
    "              [0.2,  0.5]])  # 3x2 matrix defines plane orientation\n",
    "\n",
    "X_3D = X_2D @ W.T\n",
    "\n",
    "# Add a little Gaussian noise to move points slightly off the plane\n",
    "X_3D += 0.05 * np.random.randn(n_samples, 3)\n",
    "\n",
    "# 2. Center the data\n",
    "X_centered = X_3D - X_3D.mean(axis=0)\n",
    "\n",
    "U, s, Vt = np.linalg.svd(X_centered) #this applies svd\n",
    "\n",
    "c1 = Vt[0]\n",
    "c2 = Vt[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910cf11",
   "metadata": {},
   "source": [
    "PCA assumes that the dataset is centered around the origin. As you will see, Scikit-Learn‚Äôs PCA classes take care of centering the data for you. If you implement PCA yourself (as in the preceding example), or if you use other libraries, don‚Äôt forget to center the data first.\n",
    "\n",
    "\n",
    "\n",
    "projecting down to d dimensions :\n",
    "\n",
    "after identifying the principal components we can reduce the dimensionality of the dataset down to d dimensions by projecting it onto the hyperplane defined by the first d principal components \n",
    "for eg : \n",
    "suppose ur dataset has n features(dimensions)\n",
    "in mnist each image is of 784 pixel features so n = 784 , it is in 784 d \n",
    "when we apply pca we can choose to reduce the dataset to d dimensions \n",
    "d = number of principal components you keep \n",
    "if your data is 3D (ùëõ=3) and you choose d=2:\n",
    "‚Üí You project the data onto the plane defined by the first 2 principal components.\n",
    "‚Üí Output = 2D dataset.\n",
    "\n",
    "to project the training set onto the hyperplane and obtain a reduced dataset X(d proj) of dimensionality d, compute the matrix multiplication of the training set X(m,n) by the matrix W(n,d its columns are the first d prinicipal components)\n",
    "each pc is an axis in the new reduced space \n",
    "we will get X(m,d)  \n",
    "\n",
    "PYTHON CODE :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106175e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = Vt[:2].T\n",
    "x2d = X_centered @ w2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bcbb83",
   "metadata": {},
   "source": [
    "this is how u project the dataset into a lower dimension without losing much variance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79de8df",
   "metadata": {},
   "source": [
    "Using Scikit Learn :\n",
    "\n",
    "scikit learn PCA class uses a SVD to implement the pca \n",
    "the code to apply pca to reduce it to 2d : (it automatically takes care of centering the data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc2f1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "x2d = pca.fit_transform(X_3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9890e1",
   "metadata": {},
   "source": [
    "after fitting the pca transformer to the dataset its components_ attribute holds the transpose of Wd it contains one row for each of the first d principal components \n",
    "\n",
    "\n",
    "explained variance ratio :\n",
    "\n",
    "another info we can get is the explained variance ratio of each principal component we can access it by explained_variance_ratio_ variable, it tells us about the proportion of datasets variance that lies along each principal component \n",
    "lets look at the explained variance ratios of the first two components of the 3d dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33ffcad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75330001, 0.2444161 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c111dde8",
   "metadata": {},
   "source": [
    "1st component = 75% data\n",
    "2nd componenet = 24 % data \n",
    "\n",
    "this leaves the 1 % approx data for the third dimension so it is reasonable to assume that the third pc is probably carries little information \n",
    "\n",
    "\n",
    "choosing the right no of dimensions :\n",
    "\n",
    "instead of randomly choosing the dimension to reduce we can follow the rule that the reduce dimension must contain 95% of the the variance, unless you want DR for data visualizations then you have to reduce it to 2d or 3d \n",
    "\n",
    "the following code uses the mnist dataset and performs the pca without reducing the dimensionality then computes the minimum number of dimensions required for 95% of the variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf27ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False)\n",
    "x_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\n",
    "x_test, y_test = mnist.data[60_000:], mnist.target\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(x_train) #pca is calculated on all 784 components, it also calculates how much variance each component explains \n",
    "\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #explained variance ratio is a fraction of each component explains \n",
    "#np.cumsum adds them cumulatively, eg :\n",
    "# explained_variance_ratio_ = [0.10, 0.08, 0.06, ...]\n",
    "#cumsum = [0.10, 0.18, 0.24, ...]\n",
    "\n",
    "d = np.argmax(cumsum >= 0.95) + 1 #We want the smallest d such that the cumulative explained variance ‚â• 0.95 (i.e., 95%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b468986",
   "metadata": {},
   "source": [
    "we can set the n_components = d and run pca again but the better option is to specify the variance we want to preserve you can set the n_components to be a float between 0.0 and 1.0 telling us the ratio of variance we want to preserve \n",
    "\n",
    "this removes the need to manually calculate the cumsum or d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "844282a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "x_reduced = pca.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0fb10",
   "metadata": {},
   "source": [
    "the actual no of components is determined during training and it is stored in the n_components attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "506138a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87809f",
   "metadata": {},
   "source": [
    "explained variance curve : see fig 8.8\n",
    "\n",
    "another approach is to plot the explained variance curve using dimensions in the x axis and the variance it captures in the y axis \n",
    "\n",
    "there will be an elbow in the curve which tells us after a particular dimension theres not much increase in the variance \n",
    "as we can see reducing our dataset down to 100 dimensions didnt lose much explained variance \n",
    "\n",
    "\n",
    "if we want to use the DR for preprocessing in the supervised learning task then you can tune the no of dimensions as you would do with any other hyperparameters \n",
    "\n",
    "lets see the end to end pipeline for this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2fd31af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[(&#x27;pca&#x27;, PCA(random_state=42)),\n",
       "                                             (&#x27;randomforestclassifier&#x27;,\n",
       "                                              RandomForestClassifier(random_state=42))]),\n",
       "                   param_distributions={&#x27;pca__n_components&#x27;: array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "       27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n",
       "       44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n",
       "       6...\n",
       "       414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426,\n",
       "       427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
       "       440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452,\n",
       "       453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
       "       466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n",
       "       479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491,\n",
       "       492, 493, 494, 495, 496, 497, 498, 499])},\n",
       "                   random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomizedSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">?<span>Documentation for RandomizedSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[(&#x27;pca&#x27;, PCA(random_state=42)),\n",
       "                                             (&#x27;randomforestclassifier&#x27;,\n",
       "                                              RandomForestClassifier(random_state=42))]),\n",
       "                   param_distributions={&#x27;pca__n_components&#x27;: array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "       27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n",
       "       44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n",
       "       6...\n",
       "       414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426,\n",
       "       427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
       "       440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452,\n",
       "       453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
       "       466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n",
       "       479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491,\n",
       "       492, 493, 494, 495, 496, 497, 498, 499])},\n",
       "                   random_state=42)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: Pipeline</label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=57, random_state=42)),\n",
       "                (&#x27;randomforestclassifier&#x27;,\n",
       "                 RandomForestClassifier(n_estimators=475, random_state=42))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;PCA<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.decomposition.PCA.html\">?<span>Documentation for PCA</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>PCA(n_components=57, random_state=42)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(n_estimators=475, random_state=42)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('pca', PCA(random_state=42)),\n",
       "                                             ('randomforestclassifier',\n",
       "                                              RandomForestClassifier(random_state=42))]),\n",
       "                   param_distributions={'pca__n_components': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "       27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n",
       "       44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n",
       "       6...\n",
       "       414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426,\n",
       "       427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
       "       440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452,\n",
       "       453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
       "       466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n",
       "       479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491,\n",
       "       492, 493, 494, 495, 496, 497, 498, 499])},\n",
       "                   random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#build pipeline \n",
    "clf = make_pipeline(\n",
    "    PCA(random_state=42),\n",
    "    RandomForestClassifier(random_state=42)\n",
    ")\n",
    "\n",
    "#define hyperparam search space \n",
    "param_distrib = {\n",
    "    \"pca__n_components\": np.arange(10, 80),\n",
    "    \"randomforestclassifier__n_estimators\" : np.arange(50, 500)\n",
    "}\n",
    "\n",
    "#now we will run randomized search cv to find the best pair of hyperparam \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    clf, param_distrib,\n",
    "    n_iter=10, cv=3, random_state=42\n",
    ")\n",
    "rnd_search.fit(x_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac05405",
   "metadata": {},
   "source": [
    "lets look at the best param found in the given range :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "820d3b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'randomforestclassifier__n_estimators': 475, 'pca__n_components': 57}\n"
     ]
    }
   ],
   "source": [
    "print(rnd_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c824a",
   "metadata": {},
   "source": [
    "as you can see the optimal no of dimensions required is reeduced to 57 dimensions, point to be noted is that we used a random forest which is a powerful model if we use linear model instead such as sgd classifier the search would find we need more dimensions about 70\n",
    "\n",
    "so we didnt determined the percentage of variance we want to preserve coz using randomized search cv we are directly testing what will be the best for the model \n",
    "Instead of saying ‚ÄúI want to preserve 95% variance,‚Äù\n",
    "You‚Äôre saying ‚ÄúI want to preserve just enough variance to maximize classification accuracy.‚Äù\n",
    "\n",
    "we are restricting our search space to 80 only coz this is just an example to run quickly \n",
    "we can know if the sol is optimal or not if our best_params range is in the upper boundary of your search \n",
    "\n",
    "the optimal approach is :\n",
    "first see the elbow where theres not much change in the explained variance and keep the range around it r= 50 to 200\n",
    "then zoom in once you get the best hyperparam r = 120 to 180 , this will give the most optimal sol \n",
    "\n",
    "\n",
    "PCA FOR COMPRESSION :\n",
    "\n",
    "after DR we can see the training space takes much less space, after applying pca to mnist we are left with 154 features instead of original 784 features so the dataset is now less than 20 % of original size and we lost only 5% of the variance \n",
    "\n",
    "it is lso possible to decompress the reduced data using the inverse transformation of the pca projection \n",
    "this wont give back the orginal data as it has lost 5% of the variance but it will be likely close to original data \n",
    "the mean squared distance between the original and the reconstructed data is called the reconstruction error \n",
    "\n",
    "we can use inverse_transform() for this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "448a9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_recovered = pca.inverse_transform(x_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a4c883",
   "metadata": {},
   "source": [
    "fig 8-9 shows the compressed data on the right and the quality got reduced the data remains almost the same \n",
    "\n",
    "the mathematical eq for inverse transformation to get back to the original no of features see copy 2\n",
    "\n",
    "\n",
    "RANDOMIZED PCA :\n",
    "\n",
    "pca uses a mathematical process called svd to compute the principal components the whole process is very computationally expensive when ur dataset has many features(n, high dimensions) and many samples(m, lots of rows) the time complexity is O(m x n^3) +O(n^3) \n",
    "so as the no of features grows the time comp increases dramatically \n",
    "\n",
    "to tackle this we use randomized pca which basically approxiamates the components which have more variance by using random projections to quickly estimate the subspace that captures most of the variance \n",
    "this reduces time complexity to : O(m x d^2) + O(d^3) where d is the no of principal components you want to keep \n",
    "so when d is much smaller than n this approach is faster than svd \n",
    "\n",
    "eg :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24a9e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\", random_state=42)\n",
    "x_reduced = rnd_pca.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729c8e47",
   "metadata": {},
   "source": [
    "now by default svd_solver is actually set to \"auto\" in scikit learn, it automatically uses the randomized search cv when the :\n",
    "max(m x n) > 500 or \n",
    "n_components is an integer smaller than min(m x n) \n",
    "which was in our previous example 154 < 0.8 x 784\n",
    "so no need to actually set the svd_solver in previous case \n",
    "however if you want to force use the full svd for a slightly more precise result set the svd_solver hyperparameter to \"full\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "INCREMENTAL PCA :\n",
    "\n",
    "one problem with the pca is you need to fit the whole training set into the memory in order for the algorithm to run so incremental pca was developed where you feed the training set in the form of mini batches at a time to the algorithm \n",
    "this is useful for large datasets and also in online learning where new instances keeps arriving \n",
    "\n",
    "eg :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "639273c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for x_batches in np.array_split(x_train, n_batches): #splits the tr set to 100 batches\n",
    "    inc_pca.partial_fit(x_batches) #here instead of fitting the whole dataset we are partially fitting the dataset so it updates the mean, variance and principal components incrementally\n",
    "\n",
    "x_reduced = inc_pca.transform(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f5784a",
   "metadata": {},
   "source": [
    "normally when we load an array it should fit entirely into the ram\n",
    "so alternatively we can use np.memmap to solve this :\n",
    "it stores the array on disk in a binary file \n",
    "loading only the chunks needed into memory on demand \n",
    "treats disk storage as a numpy arrray\n",
    "\n",
    "eg :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8b16e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"my_mnist.mmap\"\n",
    "\n",
    "x_mmap = np.memmap(filename, dtype='float32', mode='write', shape=x_train.shape)\n",
    "#this acts like a numpy array but the actual data lives on the disk\n",
    "# and is loaded into the memory only when needed\n",
    "#mode = write , opens file for writing \n",
    "#shape is used to tell the size of the memeapp array\n",
    "\n",
    "x_mmap[:] = x_train # copies the full mnist training data into the memory file \n",
    "x_mmap.flush() # this ensure that any data left in the write buffer is fully saved to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d918f6b",
   "metadata": {},
   "source": [
    "what happens on disk :\n",
    "the file created stores all 60000 x 784 floating points values in efficient binary format but accesible as numpy array with tiny memory footprint\n",
    "\n",
    "now we can load the memeapp file and use it like a regular numpy array , lets apply incemental pca on this, now we dont need to partially fit it as memory usage remains under control, just call the fit function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23955b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IncrementalPCA(batch_size=60000, n_components=154)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;IncrementalPCA<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.decomposition.IncrementalPCA.html\">?<span>Documentation for IncrementalPCA</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>IncrementalPCA(batch_size=60000, n_components=154)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "IncrementalPCA(batch_size=60000, n_components=154)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mmap = np.memmap(filename, dtype='float32', mode='readonly').reshape(-1, 784)\n",
    "batch_size = x_mmap.shape[0]\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(x_mmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b501a14f",
   "metadata": {},
   "source": [
    "only the raw binary data is stored in the disk so we need to specify the data type and shape of the array when you load it, if you remove the shape attribute it will return a 1d array \n",
    "\n",
    "\n",
    "for high dimensional datasets when you have tens of thousands of features applying pca is a heavy task computationally even if you go for randomized pca the no of dimensions must not be too large, in this type of cases one should use random projection instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4bbfa5",
   "metadata": {},
   "source": [
    "RANDOM PROJECTION :\n",
    "\n",
    "as the name says random projection projects the data to lower dimensions using random linear projections, it may sound unreal but two similar instances will remain similar and two different instances will remain different \n",
    "\n",
    "when we reduce the dimensions obvviously some information will be lost but if you reduce too much :\n",
    "distances betweenn points distort \n",
    "clusters break \n",
    "nearest neighbours swap \n",
    "geometry collapses \n",
    "\n",
    "so to tackle this johnson lindenstrauss lemma comes in which says :\n",
    "to preserve squared distances between any pair of points within a distortion factor Œµ with high probability you must project into atleast : see copy 3\n",
    "where m is no of samples \n",
    "Œµ is allowable distortion \n",
    "one thing to notice is we are not using n(original dimension)\n",
    "this gives us conclusion that no matter how high your dimension is you can compress it safely using m and Œµ, only the no of points matter for preserving pairwise distances this is why it si , eg :\n",
    "m = 5000\n",
    "n = 20000 features\n",
    "Œµ = 0.1 (10%)\n",
    "put this in formula we will get d = 7300, this is a significant reduction while preserving the pairwise distances within +-10%\n",
    "\n",
    "using scikit learn :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b09e1755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7300"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "m, Œµ = 5_000, 0.1\n",
    "d = johnson_lindenstrauss_min_dim(m, eps=Œµ)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447729b3",
   "metadata": {},
   "source": [
    "now we can generate a random matrix p of shape [d,n] where each item is sampled randomly from a gaussian distribution with mean 0 and variance 1/d and use it to project a dataset from n dimensions down to d :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65787d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20_000\n",
    "np.random.seed(42)\n",
    "p = np.random.randn(d,n) / np.sqrt(d) #jl lemma requires each entries should have variance 1/d\n",
    "x = np.random.randn(m,n)\n",
    "x_reduced = x @ p.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e5e71e",
   "metadata": {},
   "source": [
    "and thats it the algorithm does not requires any training only random matrix in datasets shape the data itself is not used at all \n",
    "\n",
    "now scikit learn also provide a gaussianrandomProjection class to perform this,\n",
    "it also uses jl lemma to determine the specific no of dimensions idle for the dataset then generates a random matrix which it stores in the components_ attribute, then u call transform(), it uses this matrix to perform DR, u can also set Œµ using the eps attribute and n_components attribute if you want to force a specific d for DR \n",
    "eg : (it will give the same output as the previous one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91166945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "gaussian_rnd_proj = GaussianRandomProjection(eps = Œµ, random_state=42)\n",
    "x_reduced = gaussian_rnd_proj.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6995b61a",
   "metadata": {},
   "source": [
    "sk learn also provide a second random transformer for this called SparseRandomProjection it determines the target dimensionality in the same way and then makes a random matrix \n",
    "the main difference is the matrix generated is sparse meaning it will take much smaller space about 25 mb instead of 1.2 gb which makes it faster \n",
    "and if the input is sparse the transformer keeps it sparse unless you manually set dense_output = true \n",
    "it has the same propeties as the previous one so in short its better to use this especially when your dataset is large and sparse \n",
    "\n",
    "in sparse random projection the matrix is of shape dxn and very few entries are non zero and the density r tells us what fraction of entries are non zero in the random matrix, the default density is 1/(root n) \n",
    "so for 20000 r is 141 means only 1 entry in 141 entries is non zero \n",
    "\n",
    "we need such low density as :\n",
    "sparse matrix saves massive memory and multiplying them is faster \n",
    "high sparsity = more speed with minimal accuracy lost \n",
    "\n",
    "each non zero value is randomly +v or -v where v = see copy 4 \n",
    "\n",
    "usually you cannot inverse the random projection but if you want to you can create an approxiamation inverse by computing the pseudo inverse of the components matrix, then multiply the reduced data by the transpose of the pseudo inverse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e51cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_pinv = np.linalg.pinv(gaussian_rnd_proj.components_)\n",
    "x_recovered = x_reduced @ components_pinv.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae31a472",
   "metadata": {},
   "source": [
    "so random projection is a simple fast and memory friendly DR algo that one should keep in mind while dealing with high dim datasets \n",
    "\n",
    "random projections are not only use for reducing dim but also for increasing dim while making the data small \n",
    "this is what happens in a fruit fly brain :\n",
    "1. the insect smells an odour from env it fires many neurons , few zeroes and low dim \n",
    "2. the fly has 2000 kenyons cells approx, each kenyon cells recieve an input from random subset of input neurons like generating an sparse matrix \n",
    "3. final output is a high dim sparse output and binary(on/off firing) where similar odours overlap sets of kenyon cells and different ones activate a very different sets exaclty like hash functions that preserves similarity \n",
    "\n",
    "this is very similar to locally sensitive hashing which is typically used on search engines to group similar documents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6c9a4",
   "metadata": {},
   "source": [
    "LLE\n",
    "\n",
    "locally linear embedding is a non linear dimensionality reduction NLDR method designed to uncover the hidden manifold structure of high dimensional data, it does not use linear projections instead it focuses on preserving the local geometry, this makes them very powerfull for unrolling curved or twisted manifolds such as swiss roll etc\n",
    "first they check how each training instances linearly relates to its nearest neighbours \n",
    "then it checks for the low dimensions where these local relationships are well preserved \n",
    "\n",
    "eg :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "x_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "lie = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "x_unrolled = lie.fit_transform(x_swiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8110bea3",
   "metadata": {},
   "source": [
    "the variable t is a 1D numpy array containing the position of each instance along the swiss roll, we are not using it rn but we can use it as target fot non linear regression tasks \n",
    "\n",
    "the resulting 2d dataset for this was shown in fig 8-10 , but its slighlty curved it should be a proper rectangle but the results are satisfactory\n",
    "\n",
    "working :\n",
    "\n",
    "1. for each instance lets say x(i) the algorithm find its k nearest neighbours and then express x(i) as their weighted linear combination with weights that sums to 1 using the simplest possible linear model, this captures the local geometry of the manifold see eq 8-4 see copy 5\n",
    "\n",
    "we want to minimise eq 4 subject to constraints :\n",
    "only neighbours get non zero weights, each row of weights involve only the k nearest weights all others should be 0\n",
    "and secondly all the weights sum must be 1, we find weights such that x(i) can be reconstructed at best from them \n",
    "\n",
    "2. now the second step is to map the training instances in the lower dimensional space d where d < n while preserving these local relationship as much as possible\n",
    "\n",
    "if z(i) is the image of x(i) in d dimension then we want the squared distance between z(i) and the linear relation in eq 8-4 (right side after the subtraction) to be as small as possible \n",
    "\n",
    "this leads to eq 8-5 see copy 7 where instead of keeping the instances fixed and finding the optimal weightes as seen in eq8-4 we are doing the oppisite by keeping the weights fixed and finding the optimal position for the instances in low dimensional space \n",
    "\n",
    "\n",
    "computational complexity ;\n",
    "1. O(m log(m) n log(k)) for finding the k nearest neighbours\n",
    "2. O(mnk^3) for optimizing the weights \n",
    "3. O(dm^2) for constructing low dimensional representations, the m^2 in the last makes this alorithm scale poorly on very large datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4051e12",
   "metadata": {},
   "source": [
    "OTHER DIMENSIONALITY REDUCTION TECHNIQUES :\n",
    "\n",
    "sklearn.manifold.MDS :\n",
    "multidimensional scaling MDS reduces dimensionality while trying to preserve the distances between the instances works well on low dimensional data unlike random proj which is better for high dimensional dataset \n",
    "\n",
    "sklearn.manifold.Isomap :\n",
    "it creates a graph connecting each instance to its nearest neighbours then tries to reduce the dimensionality by tring to preserve the geodesic distance between the instanes\n",
    "the geodesic distance is basically the no of nodes between two nodes when we take the shortest path \n",
    "\n",
    "sklearn.manifold.TSNE :\n",
    "t-distributed stochastic neighbour embedding t-SNE reduces the dimensionality while trying to keep the similar instances together and dissimilar instances apart, it is mostly used for visulaizations in particular to visualize the clusters in higher dimensions \n",
    "\n",
    "sklearn.discrimant_analysis.LinearDiscrimantAnalysis :\n",
    "it is a linear classification algorithm that during training learns the most discriminative axis betweenn the classes this later can be used to define a hyperplane where we can project the data \n",
    "the benefit is that after projection the classes will be seperated, it is a good technique to apply before any another classiification algorithm \n",
    "\n",
    "see fig 8-11, the techniques was applied on swiss roll "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
